<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Publications | Zhen Wang</title>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
  <meta name="author" content="Zhen Wang" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,500;0,600;0,700;1,400&family=Source+Sans+3:wght@400;500;600;700&display=swap" rel="stylesheet">
  
  <style>
    :root {
      --ucsd-navy: #182B49;
      --ucsd-blue: #00629B;
      --ucsd-yellow: #FFCD00;
      --ucsd-gold: #C69214;
      --ucsd-sand: #F5F0E6;
      --primary: var(--ucsd-navy);
      --accent: var(--ucsd-gold);
      --accent-bright: var(--ucsd-yellow);
      --secondary: var(--ucsd-blue);
      --text: #2d3748;
      --text-light: #4a5568;
      --bg: #fafafa;
      --bg-card: #ffffff;
      --border: #e2e8f0;
      --highlight-blue: #e6f3fa;
      --highlight-gold: #fff9e6;
      --highlight-red: #fef2f2;
    }

    * { margin: 0; padding: 0; box-sizing: border-box; }
    html { scroll-behavior: smooth; }
    body {
      font-family: 'Source Sans 3', -apple-system, sans-serif;
      background: var(--bg);
      color: var(--text);
      line-height: 1.7;
      font-size: 17px;
    }

    body::before {
      content: '';
      position: fixed;
      top: 0; left: 0;
      width: 100%; height: 100%;
      pointer-events: none;
      opacity: 0.03;
      background-image: url("data:image/svg+xml,%3Csvg viewBox='0 0 256 256' xmlns='http://www.w3.org/2000/svg'%3E%3Cfilter id='noise'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.9' numOctaves='4' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='100%25' height='100%25' filter='url(%23noise)'/%3E%3C/svg%3E");
    }

    .container { max-width: 1000px; margin: 0 auto; padding: 0 24px; }

    .nav {
      background: var(--ucsd-navy);
      position: sticky;
      top: 0;
      z-index: 100;
    }
    .nav-inner {
      display: flex;
      justify-content: center;
      gap: 8px;
      padding: 12px 0;
      flex-wrap: wrap;
    }
    .nav a {
      font-size: 15px;
      font-weight: 600;
      color: rgba(255, 255, 255, 0.85);
      text-decoration: none;
      padding: 8px 18px;
      border-radius: 4px;
      transition: all 0.2s ease;
      letter-spacing: 0.02em;
    }
    .nav a:hover, .nav a.active {
      background: rgba(255, 255, 255, 0.1);
      color: var(--ucsd-yellow);
    }

    .page-header {
      padding: 40px 0 24px;
      text-align: center;
    }
    .page-header h1 {
      font-family: 'Crimson Pro', Georgia, serif;
      font-size: 2.4rem;
      font-weight: 600;
      color: var(--primary);
      margin-bottom: 8px;
      letter-spacing: -0.02em;
    }
    .page-header p {
      color: var(--text-light);
      font-size: 1.1rem;
    }
    .page-header p a {
      color: var(--ucsd-blue);
      text-decoration: none;
    }
    .page-header p a:hover { text-decoration: underline; }

    .view-toggle {
      display: flex;
      justify-content: center;
      gap: 8px;
      margin-bottom: 32px;
      padding: 6px;
      background: var(--bg-card);
      border-radius: 10px;
      border: 1px solid var(--border);
      width: fit-content;
      margin-left: auto;
      margin-right: auto;
    }
    .view-toggle button {
      font-family: 'Source Sans 3', sans-serif;
      font-size: 15px;
      font-weight: 600;
      padding: 10px 24px;
      border: none;
      border-radius: 6px;
      cursor: pointer;
      transition: all 0.2s ease;
      background: transparent;
      color: var(--text-light);
    }
    .view-toggle button:hover {
      background: var(--highlight-blue);
      color: var(--ucsd-blue);
    }
    .view-toggle button.active {
      background: var(--ucsd-navy);
      color: white;
    }

    .topic-nav {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: 20px 24px;
      margin-bottom: 32px;
      display: none;
    }
    .topic-nav.visible { display: block; }
    .topic-nav h3 {
      font-family: 'Crimson Pro', Georgia, serif;
      font-size: 1.1rem;
      font-weight: 600;
      color: var(--primary);
      margin-bottom: 12px;
      display: flex;
      align-items: center;
      gap: 8px;
    }
    .topic-nav h3::before { content: 'üìë'; font-size: 1rem; }
    .topic-nav-list {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
    }
    .topic-nav-list a {
      font-size: 14px;
      font-weight: 500;
      color: var(--ucsd-blue);
      text-decoration: none;
      padding: 6px 14px;
      border-radius: 20px;
      background: var(--highlight-blue);
      transition: all 0.2s ease;
      white-space: nowrap;
    }
    .topic-nav-list a:hover {
      background: var(--ucsd-blue);
      color: white;
    }

    .section { margin-bottom: 40px; }
    .section-title {
      font-family: 'Crimson Pro', Georgia, serif;
      font-size: 1.5rem;
      font-weight: 600;
      color: var(--primary);
      margin-bottom: 20px;
      padding-bottom: 10px;
      border-bottom: 2px solid var(--ucsd-yellow);
      display: flex;
      align-items: center;
      gap: 12px;
    }
    .section-title::before {
      content: '';
      width: 4px;
      height: 24px;
      background: var(--ucsd-yellow);
      border-radius: 2px;
    }

    .paper-card {
      display: flex;
      gap: 20px;
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: 20px;
      margin-bottom: 16px;
      transition: all 0.2s ease;
    }
    .paper-card:hover {
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
      border-color: var(--ucsd-blue);
    }
    .paper-image {
      flex-shrink: 0;
      width: 160px;
      height: 100px;
      border-radius: 6px;
      overflow: hidden;
      background: var(--highlight-blue);
      display: flex;
      align-items: center;
      justify-content: center;
    }
    .paper-image img {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    .paper-content { flex: 1; min-width: 0; }
    .paper-title {
      font-family: 'Crimson Pro', Georgia, serif;
      font-size: 1.15rem;
      font-weight: 600;
      color: var(--primary);
      margin-bottom: 6px;
      line-height: 1.4;
    }
    .paper-title a { color: inherit; text-decoration: none; }
    .paper-title a:hover { color: var(--ucsd-blue); }
    .paper-authors {
      font-size: 14px;
      color: var(--text-light);
      margin-bottom: 6px;
      line-height: 1.5;
    }
    .paper-authors strong { color: var(--primary); font-weight: 600; }
    .paper-venue {
      font-size: 14px;
      color: var(--ucsd-blue);
      font-weight: 500;
      margin-bottom: 8px;
    }
    .paper-venue .highlight {
      background: var(--highlight-gold);
      color: var(--ucsd-gold);
      padding: 2px 8px;
      border-radius: 4px;
      font-weight: 600;
      font-size: 13px;
      margin-left: 8px;
    }
    .paper-venue .highlight.red {
      background: var(--highlight-red);
      color: #dc2626;
    }
    .paper-description {
      font-size: 14px;
      color: var(--text);
      line-height: 1.6;
      margin-bottom: 10px;
    }
    .paper-links {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
    }
    .paper-links a {
      font-size: 13px;
      font-weight: 500;
      color: var(--ucsd-blue);
      text-decoration: none;
      padding: 4px 12px;
      border-radius: 4px;
      background: var(--highlight-blue);
      transition: all 0.2s ease;
    }
    .paper-links a:hover {
      background: var(--ucsd-blue);
      color: white;
    }

    .footer {
      padding: 28px 0;
      text-align: center;
      border-top: 1px solid var(--border);
      margin-top: 40px;
    }
    .footer-credit { font-size: 13px; color: var(--text-light); }

    .view-section { display: none; }
    .view-section.active { display: block; }
    .topic-section { scroll-margin-top: 80px; }

    @media (max-width: 768px) {
      .paper-card { flex-direction: column; }
      .paper-image { width: 100%; height: 140px; }
      .page-header h1 { font-size: 1.8rem; }
      .view-toggle { flex-direction: column; width: 100%; }
      .view-toggle button { width: 100%; }
      .topic-nav-list { flex-direction: column; }
      .topic-nav-list a { text-align: center; }
    }

    @keyframes fadeInUp {
      from { opacity: 0; transform: translateY(20px); }
      to { opacity: 1; transform: translateY(0); }
    }
    .page-header { animation: fadeInUp 0.6s ease-out; }
    .section { animation: fadeInUp 0.6s ease-out backwards; }
  </style>
</head>

<body>
  <div style="height: 4px; background: linear-gradient(90deg, var(--ucsd-yellow), var(--ucsd-gold));"></div>
  
  <nav class="nav">
    <div class="container nav-inner">
      <a href="/index.html">About</a>
      <a href="/page_research.html">Research</a>
      <a href="/page_pubs.html" class="active">Publications</a>
      <a href="/page_posts.html">Blog</a>
      <a href="/page_honors_and_services.html">Honors & Service</a>
    </div>
  </nav>

  <main class="container">
    <header class="page-header">
      <h1>Publications</h1>
      <p>*=co-first author &nbsp;|&nbsp; ‚Ä†=corresponding author &nbsp;|&nbsp; <a href="https://scholar.google.com/citations?hl=en&user=asBaytUAAAAJ&view_op=list_works" target="_blank">Google Scholar ‚Üí</a></p>
    </header>

    <div class="view-toggle">
      <button class="active" onclick="switchView('years')">üìÖ By Years</button>
      <button onclick="switchView('topics')">üè∑Ô∏è By Topics</button>
    </div>

    <div class="topic-nav" id="topicNav">
      <h3>Quick Navigation</h3>
      <div class="topic-nav-list">
        <a href="#topic-1">Knowledge-Structured Representation</a>
        <a href="#topic-2">Efficient Training & Adaptation</a>
        <a href="#topic-3">Agentic Reasoning & Planning</a>
        <a href="#topic-4">Human-Aligned Learning & Evaluation</a>
        <a href="#topic-5">Scientific AI & Discovery</a>
      </div>
    </div>

    <!-- ==================== BY YEARS VIEW ==================== -->
    <div class="view-section active" id="viewYears">

      <!-- 2026 -->
      <section class="section">
        <h2 class="section-title">2026</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/firebench_2025.png" alt="FIRE-Bench" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://firebench.github.io/">FIRE-Bench: Evaluating Research Agents on the Rediscovery of Scientific Insights</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>*, Fan Bai*, Zhongyan Luo*, Jinyan Su, Kaiser Sun, Xinle Yu, Jieyuan Liu, Kun Zhou, Claire Cardie, Mark Dredze, Eric P. Xing, Zhiting Hu</p>
            <p class="paper-venue">In submission to ICML 2026</p>
            <p class="paper-description">A benchmark for reliably evaluating research agents on their ability to rediscover scientific insights.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF</a>
            </div>
          </div>
        </div>
        

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/simulating_humans_2026.png" alt="Simulating Humans" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">Simulating Humans for Personalized Language Modeling</a></h3>
            <p class="paper-authors">Jinzhou Tang, Yufan Zhou, Zixuan Wang, Xinle Yu, Zhaoxiang Feng, Steven Ngo, Zhengding Hu, Luoshang Pan, Lianhui Qin, Yufei Ding, Tianmin Shu, Jingbo Shang, Zhiting Hu, <strong>Zhen Wang</strong></p>
            <p class="paper-venue">In submission to ICML 2026</p>
            <p class="paper-description">A framework for simulating human behavior to enable personalized language modeling.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/hypoevolve_2026.png" alt="HypoEvolve" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">HypoEvolve: When Genetic Algorithm Meets Multi-Agents for Discovering Scientific Hypothesis</a></h3>
            <p class="paper-authors">Jieyuan Liu, Mengzhou Hu, Jefferson Chen, Hsin-Yuan Lee, Dexter Pratt, Lianhui Qin, Trey Ideker, Zhiting Hu, Eric P. Xing, Wei Wang, <strong>Zhen Wang</strong></p>
            <p class="paper-venue">In submission to ICML 2026</p>
            <p class="paper-description">A novel approach combining genetic algorithms with multi-agent systems for automated scientific hypothesis discovery.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/nabla_2025.png" alt="Nabla Reasoner" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">Nabla Reasoner: LLM Reasoning via Test-Time Gradient Descent in Textual Space</a></h3>
            <p class="paper-authors">Peihao Wang, Ruisi Cai, <strong>Zhen Wang</strong>, Hongyuan Mei, Qiang Liu, Pan Li, Zhangyang Wang</p>
            <p class="paper-venue">ICLR 2026</p>
            <p class="paper-description">A novel approach to LLM reasoning through test-time gradient descent operations in textual space.</p>
            <div class="paper-links">
              <a href="https://openreview.net/pdf?id=pEJAja73dk">PDF</a>
            </div>
          </div>
        </div>
        
        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/tritondft_2026.png" alt="TritonDFT" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">TritonDFT: Automating DFT with a Multi-Agent Framework</a></h3>
            <p class="paper-authors">Zhengding Hu, Kuntal Talit, <strong>Zhen Wang</strong>, Haseeb Ahmad, Yichen Lin, Prabhleen Kaur, Christopher Lane, Elizabeth A. Peterson, Zhiting Hu, Elizabeth A. Nowadnick, Yufei Ding</p>
            <p class="paper-venue">In submission to ICML 2026</p>
            <p class="paper-description">A multi-agent framework for automating Density Functional Theory (DFT) calculations.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        
        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/m3_memory_2025.png" alt="M3 Memory" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">M¬≥: Multi-Tier Memory Managing System for Agentic LLM Serving</a></h3>
            <p class="paper-authors">Zhengding Hu, Zaifeng Pan, Prabhleen Kaur, Vibha Murthy, Zhongkai Yu, Yue Guan, <strong>Zhen Wang</strong>, Steven Swanson, Yufei Ding</p>
            <p class="paper-venue">In submission to OSDI 2026</p>
            <p class="paper-description">A multi-tier memory managing system designed for efficient agentic LLM serving.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>
        
      </section>

      <!-- 2025 -->
      <section class="section">
        <h2 class="section-title">2025</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/scpilot_neurips_2025.png" alt="scPilot" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery</a></h3>
            <p class="paper-authors">Yiming Gao*, <strong>Zhen Wang</strong>*‚Ä†, Jefferson Chen, Mark Antkowiak, Mengzhou Hu, JungHo Kong, Dexter Pratt, Jieyuan Liu, Enze Ma, Zhiting Hu, Eric P. Xing</p>
            <p class="paper-venue">NeurIPS 2025</p>
            <p class="paper-description">The first omics-native reasoning agent that grounds LLMs in raw single-cell data for automated analysis and biological discovery.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="https://images.zapnito.com/uploads/6178f6ad93e0f121c43c100c57f43dba/4c6b1657-214e-439d-bca4-14c07a01ed72.png" alt="Nature 2025" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://www.biorxiv.org/content/10.1101/2023.01.03.522354">Atlas-Guided Discovery of Transcription Factors for T Cell Programming</a></h3>
            <p class="paper-authors">H. Kay Chung, Cong Liu, Anamika Battu, ... <strong>Zhen Wang</strong>, Jieyuan Liu, Yiming Gao, Zhiting Hu, ... Wei Wang</p>
            <p class="paper-venue">Nature 2025 <span class="highlight red">(In Press)</span></p>
            <p class="paper-description">Contributed TaijiChat, a Paper Copilot for multi-omics discovery of transcription factors for T cell programming.</p>
            <div class="paper-links">
              <a href="https://www.biorxiv.org/content/10.1101/2023.01.03.522354v6.full.pdf">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/bioarchive-logo.png" alt="MutationProjector" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://www.biorxiv.org/content/10.1101/2025.09.08.674723">A Foundation Model of Cancer Genotype Enables Precise Predictions of Therapeutic Response</a></h3>
            <p class="paper-authors">JungHo Kong, Ingoo Lee, Dean Boecher, Akshat Singhal, Marcus Kelly, Jimin Moon, Chang Ho Ahn, Chan-Young Ock, Tannavee Kumar, Timothy John Sears, David Laub, Sarah Wright, Patrick Wall, Hannah Carter, <strong>Zhen Wang</strong>‚Ä†, Trey Ideker‚Ä† (‚Ä†co-corresponding authors)</p>
            <p class="paper-venue">Under Revision at Cancer Discovery</p>
            <p class="paper-description">The first cancer genomics foundation model for tumor mutation profiles, enabling precise predictions of therapeutic response.</p>
            <div class="paper-links">
              <a href="https://www.biorxiv.org/content/10.1101/2025.09.08.674723v1.full.pdf">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/deeppersona_2025.png" alt="DeepPersona" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>*, Yufan Zhou*, Zhongyan Luo, Lyumanshan Ye, Adam Wood, Man Yao, Saab Mansour, Luoshang Pan</p>
            <p class="paper-venue">Spotlight at NeurIPS 2025 LAW</p>
            <p class="paper-description">A generative engine for creating deep synthetic personas that enable realistic human simulation at scale.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/abs/2511.07338">PDF</a>
            </div>
          </div>
        </div>


        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/cellmaster_2025.png" alt="CellMaster.AI" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">CellMaster: Collaborative Cell Type Annotation in Single-Cell Analysis</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>*, Yiming Gao*, Jieyuan Liu*, Enze Ma, Jefferson Chen, Mark Antkowiak, Mengzhou Hu, JungHo Kong, Dexter Pratt, Zhiting Hu, Wei Wang, Trey Ideker, Eric P. Xing</p>
            <p class="paper-venue">In submission to ISCB 2026</p>
            <p class="paper-description">A collaborative AI scientist agent for automated cell type annotation in single-cell transcriptomics analysis.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/modal_cot_2025.png" alt="Modal-mixed CoT" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">Learning Modal-mixed Chain-of-thought Reasoning with Latent Embedding</a></h3>
            <p class="paper-authors">Yifei Shao, Kun Zhou, Mohammad Atif Quamar, Ziming Xu, Shibo Hao, <strong>Zhen Wang</strong>, Zhiting Hu, Biwei Huang</p>
            <p class="paper-venue">In submission to ICLR 2026</p>
            <p class="paper-description">Learning chain-of-thought reasoning that seamlessly mixes different modalities through latent embeddings.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/ArXiv_logo_2022.png" alt="Decentralized Arena">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2505.12808">Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models</a></h3>
            <p class="paper-authors">Yanbin Yin*, Kun Zhou*, <strong>Zhen Wang</strong>*, Xiangdong Zhang, Yifei Shao, Shibo Hao, Yi Gu, Jieyuan Liu, Somanshu Singla, Tianyang Liu, Eric P. Xing, Zhengzhong Liu, Haojian Jin, Zhiting Hu</p>
            <p class="paper-venue">ArXiv; In submission to ACL 2026</p>
            <p class="paper-description">Democratic LLM benchmarking where models judge each other for scalable and fair evaluation.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2505.12808">PDF</a>
              <a href="https://de-arena.maitrix.org/">Demo</a>
              <a href="https://huggingface.co/spaces/LLM360/de-arena">Leaderboard</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/selfmoe_iclr_2025.png" alt="Self-MoE" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2406.12034">Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts</a></h3>
            <p class="paper-authors">Junmo Kang, Leonid Karlinsky, Hongyin Luo, <strong>Zhen Wang</strong>, Jacob Hansen, James Glass, David Cox, Rameswar Panda, Rogerio Feris, Alan Ritter</p>
            <p class="paper-venue">ICLR 2025</p>
            <p class="paper-description">Transforms monolithic LLMs into modular systems with self-specialized experts for compositional capabilities.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2406.12034">PDF</a>
            </div>
          </div>
        </div>

      </section>

      <!-- 2024 -->
      <section class="section">
        <h2 class="section-title">2024</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/drpo_emnlp_2024.png" alt="DRPO" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2411.08733">Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models</a></h3>
            <p class="paper-authors">Somanshu Singla*, <strong>Zhen Wang</strong>*‚Ä†, Tianyang Liu, Abdullah Ashfaq, Zhiting Hu, Eric P. Xing</p>
            <p class="paper-venue">EMNLP 2024 (Main, Long)</p>
            <p class="paper-description">First tuning-free method for self-aligning LLMs with human preferences through dynamic rewarding and prompt optimization.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2411.08733">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/fig_llm-reasoner_colm2024.png" alt="LLM Reasoners" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2404.05221">LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</a></h3>
            <p class="paper-authors">Shibo Hao*, Yi Gu*, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, <strong>Zhen Wang</strong>, Zhiting Hu</p>
            <p class="paper-venue">COLM 2024 <span class="highlight">‚≠ê 2.3k+ GitHub Stars</span></p>
            <p class="paper-description">A library enabling LLMs to conduct complex reasoning with advanced algorithms, approaching multi-step reasoning as planning with world models and rewards.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2404.05221">PDF</a>
              <a href="https://github.com/maitrix-org/llm-reasoners">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/promptagent_iclr_2024.png" alt="PromptAgent" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2310.16427">PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization</a></h3>
            <p class="paper-authors">Xinyuan Wang*, Chenxi Li*, <strong>Zhen Wang</strong>*‚Ä†, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, Zhiting Hu</p>
            <p class="paper-venue">ICLR 2024</p>
            <p class="paper-description">First principled framework to formalize API-based prompt optimization as planning with state, action, and reward; first to benchmark exploration efficiency and show transferability of optimized prompts.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2310.16427.pdf">PDF</a>
              <a href="https://github.com/XinyuanWangCS/PromptAgent">Code</a>
              <a href="https://zhenwang9102.github.io/files/promptagnet_poster_2023.pdf">Poster</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/fig_gpr_turing_machine.png" alt="GPT Turing Machine" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2303.14310">GPT Is Becoming a Turing Machine: Here Are Some Ways to Program It</a></h3>
            <p class="paper-authors">Ana Jojic, <strong>Zhen Wang</strong>, Nebojsa Jojic</p>
            <p class="paper-venue">ICLR 2024 AGI Workshop</p>
            <p class="paper-description">Through appropriate prompting, GPT models can perform iterative behaviors to execute (not just write) programs with loops, including algorithms like logical deduction, bubble sort, and LCS.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2303.14310">PDF</a>
            </div>
          </div>
        </div>

      </section>

      <!-- 2023 -->
      <section class="section">
        <h2 class="section-title">2023</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/rap_emnlp_2023.png" alt="RAP" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2305.14992">Reasoning with Language Model is Planning with World Model</a></h3>
            <p class="paper-authors">Shibo Hao*, Yi Gu*, Haodi Ma, Joshua Jiahua Hong, <strong>Zhen Wang</strong>, Daisy Zhe Wang, Zhiting Hu</p>
            <p class="paper-venue">EMNLP 2023 (Oral, Main) <span class="highlight red">Featured in State of AI Report 2023</span></p>
            <p class="paper-description">RAP reformulates LLM reasoning as a planning problem, incorporating external world models and principled planning for the best balance of exploration vs exploitation.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2305.14992.pdf">PDF</a>
              <a href="https://github.com/Ber666/llm-reasoners">Code</a>
              <a href="https://docs.google.com/presentation/d/156WpBF_rGvf4Ecg19oM1fyR51g4FAmHV3Zs0WLukrLQ/edit#slide=id.g24daeb7f4f0_0_3930">State of AI 2023</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/toolkengpt_neurips_2023.png" alt="ToolkenGPT" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2305.11554">ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings</a></h3>
            <p class="paper-authors">Shibo Hao, Tianyang Liu, <strong>Zhen Wang</strong>, Zhiting Hu</p>
            <p class="paper-venue">NeurIPS 2023 <span class="highlight red">Oral (Top 2%)</span> <span class="highlight">Best Paper @ SoCal NLP 2023</span></p>
            <p class="paper-description">Augments LLMs with massive tools/APIs by representing tools as tokens ("toolken"), enabling tool calls as naturally as generating words. Super efficient‚Äîplugging in new tools is as easy as learning embeddings.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2305.11554.pdf">PDF</a>
              <a href="https://github.com/Ber666/ToolkenGPT">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/thinksum_acl_2023.png" alt="ThinkSum" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2210.01293">ThinkSum: Probabilistic Reasoning Over Sets Using Large Language Models</a></h3>
            <p class="paper-authors">Batu Ozturkler, Nikolay Malkin, <strong>Zhen Wang</strong>, Nebojsa Jojic</p>
            <p class="paper-venue">ACL 2023 (Main)</p>
            <p class="paper-description">A two-stage probabilistic inference paradigm to improve LLMs' reasoning over multiple objects through Think (retrieval) and Sum (aggregation), beating chain-of-thought on hard BIG-bench tasks.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2210.01293.pdf">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/mpt_overview.png" alt="MPT" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2303.02861">Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, Yoon Kim</p>
            <p class="paper-venue">ICLR 2023</p>
            <p class="paper-description">Multitask Prompt Tuning (MPT) exploits rich cross-task knowledge for efficient and generalizable transfer learning through novel prompt decomposition and distillation.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2303.02861.pdf">PDF</a>
              <a href="https://github.com/huggingface/peft/pull/400">Huggingface PEFT PR</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/meet_eacl_2023.png" alt="MeeT" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2210.06444">Frustratingly Simple Entity Tracking with Effective Use of Multi-Task Learning Models</a></h3>
            <p class="paper-authors">Janvijay Singh, Fan Bai, <strong>Zhen Wang</strong></p>
            <p class="paper-venue">EACL 2023 (Main)</p>
            <p class="paper-description">Shows how to transfer multi-task knowledge from pre-training to niche downstream tasks like entity tracking, achieving SOTA by fine-tuning T5 with specialized QA prompts and task-specific decoding.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2210.06444.pdf">PDF</a>
              <a href="https://github.com/iamjanvijay/MeeT">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/sigdial_2023.png" alt="SIGDIAL 2023" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://aclanthology.org/2023.sigdial-1.29/">Roll Up Your Sleeves: Working with a Collaborative and Engaging Task-Oriented Dialogue System</a></h3>
            <p class="paper-authors">Lingbo Mo, Shijie Chen, Ziru Chen, Xiang Deng, Ashley Lewis, Sunit Singh, Samuel Stevens, Chang-You Tai, <strong>Zhen Wang</strong>, Xiang Yue, Tianshu Zhang, Yu Su, Huan Sun</p>
            <p class="paper-venue">SIGDIAL 2023</p>
            <p class="paper-description">A collaborative and engaging task-oriented dialogue system for multi-step cooking and home improvement tasks.</p>
            <div class="paper-links">
              <a href="https://aclanthology.org/2023.sigdial-1.29.pdf">PDF</a>
            </div>
          </div>
        </div>

      </section>

      <!-- 2022 -->
      <section class="section">
        <h2 class="section-title">2022</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/cb_acl_2022.png" alt="Coherence Boosting" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2110.08294">Coherence Boosting: When Your Pretrained Language Model is <span style="color: #dc2626">Not</span> Paying Enough Attention</a></h3>
            <p class="paper-authors">Nikolay Malkin, <strong>Zhen Wang</strong>, Nebojsa Jojic</p>
            <p class="paper-venue">ACL 2022 (Main, Long, Oral)</p>
            <p class="paper-description">Demonstrates that LLMs have insufficiently learned the effect of distant words on next-token prediction. Coherence Boosting increases an LM's focus on long context, greatly improving NLG and NLU tasks.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2110.08294.pdf">PDF</a>
              <a href="https://github.com/zhenwang9102/coherence-boosting">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/simultqa_2022.png" alt="SimultQA" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://aclanthology.org/2022.suki-1.7/">Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering</a></h3>
            <p class="paper-authors">Lingbo Mo*, <strong>Zhen Wang</strong>*, Jie Zhao, Huan Sun</p>
            <p class="paper-venue">NAACL 2022 SUKI Workshop</p>
            <p class="paper-description">Studies knowledge transfer for multi-hop reasoning between structured (KB) and unstructured (text) knowledge. SimultQA unifies KBQA and TextQA to study how reasoning transfers between knowledge sources.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/SimultQA_2022.pdf">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/dissertation_2022.png" alt="Dissertation" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://www.proquest.com/openview/16bbeab89e764c0f8fcc14c65df7fe9b/1?pq-origsite=gscholar&cbl=18750&diss=y">Toward Knowledge-Centric NLP: Acquisition, Representation, Transfer, and Reasoning</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong></p>
            <p class="paper-venue">Ph.D. Dissertation, The Ohio State University, 2022</p>
            <p class="paper-description">Doctoral dissertation on building foundations for knowledge-centric AI systems through acquisition, representation, transfer, and reasoning.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/Zhen_Dissertation_2022.pdf">PDF</a>
            </div>
          </div>
        </div>

      </section>

      <!-- 2021 -->
      <section class="section">
        <h2 class="section-title">2021</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/alexa_prize_figure.jpeg" alt="TacoBot" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2207.05223">Bootstrapping a User-Centered Task-Oriented Dialogue System</a></h3>
            <p class="paper-authors">Shijie Chen, Ziru Chen, Xiang Deng, Ashley Lewis, Lingbo Mo, Samuel Stevens, <strong>Zhen Wang</strong>, Xiang Yue, Tianshu Zhang, Yu Su, Huan Sun</p>
            <p class="paper-venue">Alexa Prize TaskBot Challenge 2021 <span class="highlight red">üèÜ 3rd Place Winner</span></p>
            <p class="paper-description">TacoBot, a task-oriented dialogue system for cooking and home improvement tasks. Proposes data augmentation methods including GPT-3 simulation to bootstrap neural dialogue systems into new domains.</p>
            <div class="paper-links">
              <a href="https://assets.amazon.science/9a/30/5e4931ec41d78abad730707ce95a/bootstrapping-a-user-centered-task-oriented-dialogue-system.pdf">PDF</a>
              <a href="https://www.amazon.science/alexa-prize/three-top-performers-emerge-in-inaugural-alexa-prize-taskbot-challenge">Amazon Science</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/conpi_wsdm_2021.png" alt="ConPI" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/files/WSDM2021_ZW_ConPI.pdf">Modeling Context Pair Interaction for Pairwise Tasks on Graphs</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Bo Zong, Huan Sun</p>
            <p class="paper-venue">WSDM 2021 (Long)</p>
            <p class="paper-description">Explicitly models context interactions for pairwise prediction on graphs through node-centric and pair-centric perspectives, with pre-trained pair embeddings to facilitate pair-centric modeling.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/WSDM2021_ZW_ConPI.pdf">PDF</a>
              <a href="https://github.com/zhenwang9102/ConPI">Code</a>
            </div>
          </div>
        </div>

      </section>

      <!-- 2020 -->
      <section class="section">
        <h2 class="section-title">2020</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/x-clinrela_acl__2020.png" alt="X-MedRELA" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2005.00889">Rationalizing Medical Relation Prediction from Corpus-level Statistics</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Jennifer Lee, Simon Lin, Huan Sun</p>
            <p class="paper-venue">ACL 2020 (Main, Long)</p>
            <p class="paper-description">A self-interpretable framework to rationalize neural relation prediction based on corpus-level statistics, inspired by human cognitive theory about recall and recognition, providing structured knowledge triplets as rationales.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/ACL2020_ZW_X_MedRELA.pdf">PDF</a>
              <a href="https://github.com/zhenwang9102/X-MedRELA">Code</a>
              <a href="https://slideslive.com/38929313/rationalizing-medical-relation-prediction-from-corpuslevel-statistics">Video</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/graph_bioinformatics.png" alt="BioNEV" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/1906.05017">Graph Embedding on Biomedical Networks: Methods, Applications, and Evaluations</a></h3>
            <p class="paper-authors">Xiang Yue, <strong>Zhen Wang</strong>, Jingong Huang, Srinivasan Parthasarathy, Soheil Moosavinasab, Yungui Huang, Simon Lin, Wen Zhang, Ping Zhang, Huan Sun</p>
            <p class="paper-venue">Bioinformatics, Volume 36, Issue 4, February 2020</p>
            <p class="paper-description">Benchmarks 11 representative graph embedding methods on five important biomedical tasks, verifying effectiveness and providing general guidelines for their usage.</p>
            <div class="paper-links">
              <a href="https://academic.oup.com/bioinformatics/article/36/4/1241/5581350">PDF</a>
              <a href="https://github.com/xiangyue9607/BioNEV">Code</a>
            </div>
          </div>
        </div>

      </section>

      <!-- 2019 -->
      <section class="section">
        <h2 class="section-title">2019</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/surfcon_kdd_2019.png" alt="SurfCon" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/1906.09285">SurfCon: Synonym Discovery on Privacy-Aware Clinical Data</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Xiang Yue, Soheil Moosavinasab, Yungui Huang, Simon Lin, Huan Sun</p>
            <p class="paper-venue">KDD 2019 (Research Track, Long, Oral)</p>
            <p class="paper-description">Discovers structured knowledge‚Äîsynonyms‚Äîfrom privacy-aware clinical text corpus, leveraging both surface form and context information to discover out-of-distribution synonyms.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/KDD2019_ZW_SurfCon_paper.pdf">PDF</a>
              <a href="https://github.com/zhenwang9102/SurfCon">Code</a>
              <a href="https://zhenwang9102.github.io/files/KDD2019_ZW_SurfCon_Slides.pdf">Slides</a>
              <a href="https://zhenwang9102.github.io/files/KDD2019_ZW_SurfCon_Poster.pdf">Poster</a>
            </div>
          </div>
        </div>

      </section>

      <!-- Before 2019 -->
      <section class="section">
        <h2 class="section-title">Before 2019</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/code_kdd_2018_dlday.png" alt="StaQC" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://ziyuyao.org/paper/StaQC_DLDay18.pdf">A Comprehensive Study of StaQC for Deep Code Summarization</a></h3>
            <p class="paper-authors">Jayavardhan Reddy Peddamail, Ziyu Yao, <strong>Zhen Wang</strong>, Huan Sun</p>
            <p class="paper-venue">KDD 2018 Deep Learning Day <span class="highlight">Spotlight</span></p>
            <p class="paper-description">Examines three popular datasets mined from Stack Overflow on code summarization, showing that StaQC (Stack Overflow Question-Code pairs) achieves substantially better results.</p>
            <div class="paper-links">
              <a href="https://ziyuyao.org/paper/StaQC_DLDay18.pdf">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/hessian_mmm_2015.png" alt="HessianSC" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/files/MMM2015_HessianSC.pdf">Hessian Regularized Sparse Coding for Human Action Recognition</a></h3>
            <p class="paper-authors">Weifeng Liu, <strong>Zhen Wang</strong>, Dapeng Tao, Jun Yu</p>
            <p class="paper-venue">MMM 2015</p>
            <p class="paper-description">Proposes Hessian regularized sparse coding (HessianSC) for action recognition, preserving local geometry and steering sparse coding linearly along the data manifold.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/MMM2015_HessianSC.pdf">PDF</a>
            </div>
          </div>
        </div>

      </section>

    </div>

    <!-- ==================== BY TOPICS VIEW ==================== -->
    <div class="view-section" id="viewTopics">

      <!-- Topic 1 -->
      <section class="section topic-section" id="topic-1">
        <h2 class="section-title">Knowledge-Structured Representation Learning</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/mpt_overview.png" alt="MPT" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2303.02861">Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, Yoon Kim</p>
            <p class="paper-venue">ICLR 2023</p>
            <p class="paper-description">Multitask Prompt Tuning (MPT) exploits rich cross-task knowledge for efficient and generalizable transfer learning through novel prompt decomposition and distillation.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2303.02861.pdf">PDF</a>
              <a href="https://github.com/huggingface/peft/pull/400">Huggingface PEFT PR</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/cb_acl_2022.png" alt="Coherence Boosting" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2110.08294">Coherence Boosting: When Your Pretrained Language Model is <span style="color: #dc2626">Not</span> Paying Enough Attention</a></h3>
            <p class="paper-authors">Nikolay Malkin, <strong>Zhen Wang</strong>, Nebojsa Jojic</p>
            <p class="paper-venue">ACL 2022 (Main, Long, Oral)</p>
            <p class="paper-description">Demonstrates that LLMs have insufficiently learned the effect of distant words on next-token prediction. Coherence Boosting increases an LM's focus on long context, greatly improving NLG and NLU tasks.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2110.08294.pdf">PDF</a>
              <a href="https://github.com/zhenwang9102/coherence-boosting">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/conpi_wsdm_2021.png" alt="ConPI" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/files/WSDM2021_ZW_ConPI.pdf">Modeling Context Pair Interaction for Pairwise Tasks on Graphs</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Bo Zong, Huan Sun</p>
            <p class="paper-venue">WSDM 2021 (Long)</p>
            <p class="paper-description">Explicitly models context interactions for pairwise prediction on graphs through node-centric and pair-centric perspectives, with pre-trained pair embeddings to facilitate pair-centric modeling.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/WSDM2021_ZW_ConPI.pdf">PDF</a>
              <a href="https://github.com/zhenwang9102/ConPI">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/x-clinrela_acl__2020.png" alt="X-MedRELA" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2005.00889">Rationalizing Medical Relation Prediction from Corpus-level Statistics</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Jennifer Lee, Simon Lin, Huan Sun</p>
            <p class="paper-venue">ACL 2020 (Main, Long)</p>
            <p class="paper-description">A self-interpretable framework to rationalize neural relation prediction based on corpus-level statistics, inspired by human cognitive theory about recall and recognition, providing structured knowledge triplets as rationales.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/ACL2020_ZW_X_MedRELA.pdf">PDF</a>
              <a href="https://github.com/zhenwang9102/X-MedRELA">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/surfcon_kdd_2019.png" alt="SurfCon" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/1906.09285">SurfCon: Synonym Discovery on Privacy-Aware Clinical Data</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Xiang Yue, Soheil Moosavinasab, Yungui Huang, Simon Lin, Huan Sun</p>
            <p class="paper-venue">KDD 2019 (Research Track, Long, Oral)</p>
            <p class="paper-description">Discovers structured knowledge‚Äîsynonyms‚Äîfrom privacy-aware clinical text corpus, leveraging both surface form and context information to discover out-of-distribution synonyms.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/KDD2019_ZW_SurfCon_paper.pdf">PDF</a>
              <a href="https://github.com/zhenwang9102/SurfCon">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/simultqa_2022.png" alt="SimultQA" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://aclanthology.org/2022.suki-1.7/">Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering</a></h3>
            <p class="paper-authors">Lingbo Mo*, <strong>Zhen Wang</strong>*, Jie Zhao, Huan Sun</p>
            <p class="paper-venue">NAACL 2022 SUKI Workshop</p>
            <p class="paper-description">Studies knowledge transfer for multi-hop reasoning between structured (KB) and unstructured (text) knowledge. SimultQA unifies KBQA and TextQA to study how reasoning transfers between knowledge sources.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/SimultQA_2022.pdf">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/code_kdd_2018_dlday.png" alt="StaQC" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://ziyuyao.org/paper/StaQC_DLDay18.pdf">A Comprehensive Study of StaQC for Deep Code Summarization</a></h3>
            <p class="paper-authors">Jayavardhan Reddy Peddamail, Ziyu Yao, <strong>Zhen Wang</strong>, Huan Sun</p>
            <p class="paper-venue">KDD 2018 Deep Learning Day <span class="highlight">Spotlight</span></p>
            <p class="paper-description">Examines three popular datasets mined from Stack Overflow on code summarization, showing that StaQC (Stack Overflow Question-Code pairs) achieves substantially better results.</p>
            <div class="paper-links">
              <a href="https://ziyuyao.org/paper/StaQC_DLDay18.pdf">PDF</a>
            </div>
          </div>
        </div>

      </section>

      <!-- Topic 2 -->
      <section class="section topic-section" id="topic-2">
        <h2 class="section-title">Efficient Training and Adaptation of Foundation Models</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/selfmoe_iclr_2025.png" alt="Self-MoE" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2406.12034">Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts</a></h3>
            <p class="paper-authors">Junmo Kang, Leonid Karlinsky, Hongyin Luo, <strong>Zhen Wang</strong>, Jacob Hansen, James Glass, David Cox, Rameswar Panda, Rogerio Feris, Alan Ritter</p>
            <p class="paper-venue">ICLR 2025</p>
            <p class="paper-description">Transforms monolithic LLMs into modular systems with self-specialized experts for compositional capabilities.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2406.12034">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/mpt_overview.png" alt="MPT" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2303.02861">Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, Yoon Kim</p>
            <p class="paper-venue">ICLR 2023</p>
            <p class="paper-description">Multitask Prompt Tuning (MPT) exploits rich cross-task knowledge for efficient and generalizable transfer learning through novel prompt decomposition and distillation.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2303.02861.pdf">PDF</a>
              <a href="https://github.com/huggingface/peft/pull/400">Huggingface PEFT PR</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/toolkengpt_neurips_2023.png" alt="ToolkenGPT" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2305.11554">ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings</a></h3>
            <p class="paper-authors">Shibo Hao, Tianyang Liu, <strong>Zhen Wang</strong>, Zhiting Hu</p>
            <p class="paper-venue">NeurIPS 2023 <span class="highlight red">Oral (Top 2%)</span> <span class="highlight">Best Paper @ SoCal NLP 2023</span></p>
            <p class="paper-description">Augments LLMs with massive tools/APIs by representing tools as tokens ("toolken"), enabling tool calls as naturally as generating words. Super efficient‚Äîplugging in new tools is as easy as learning embeddings.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2305.11554.pdf">PDF</a>
              <a href="https://github.com/Ber666/ToolkenGPT">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/meet_eacl_2023.png" alt="MeeT" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2210.06444">Frustratingly Simple Entity Tracking with Effective Use of Multi-Task Learning Models</a></h3>
            <p class="paper-authors">Janvijay Singh, Fan Bai, <strong>Zhen Wang</strong></p>
            <p class="paper-venue">EACL 2023 (Main)</p>
            <p class="paper-description">Shows how to transfer multi-task knowledge from pre-training to niche downstream tasks like entity tracking, achieving SOTA by fine-tuning T5 with specialized QA prompts and task-specific decoding.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2210.06444.pdf">PDF</a>
              <a href="https://github.com/iamjanvijay/MeeT">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/m3_memory_2025.png" alt="M3 Memory" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">M¬≥: Multi-Tier Memory Managing System for Agentic LLM Serving</a></h3>
            <p class="paper-authors">Zhengding Hu, Zaifeng Pan, Prabhleen Kaur, Vibha Murthy, Zhongkai Yu, Yue Guan, <strong>Zhen Wang</strong>, Steven Swanson, Yufei Ding</p>
            <p class="paper-venue">In submission to OSDI 2026</p>
            <p class="paper-description">A multi-tier memory managing system designed for efficient agentic LLM serving.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

      </section>

      <!-- Topic 3 -->
      <section class="section topic-section" id="topic-3">
        <h2 class="section-title">Agentic Reasoning and Planning with World Models</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/scpilot_neurips_2025.png" alt="scPilot" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery</a></h3>
            <p class="paper-authors">Yiming Gao*, <strong>Zhen Wang</strong>*‚Ä†, Jefferson Chen, Mark Antkowiak, Mengzhou Hu, JungHo Kong, Dexter Pratt, Jieyuan Liu, Enze Ma, Zhiting Hu, Eric P. Xing</p>
            <p class="paper-venue">NeurIPS 2025</p>
            <p class="paper-description">The first omics-native reasoning agent that grounds LLMs in raw single-cell data for automated analysis and biological discovery.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/cellmaster_2025.png" alt="CellMaster.AI" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">CellMaster.AI: A Collaborative AI Scientist Agent for Cell Type Annotation in Single-Cell Transcriptomics Analysis</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Yiming Gao, Jieyuan Liu, Mark Antkowiak, Enze Ma, Ding Bai, JungHo Kong, Mengzhou Hu, Dexter Pratt, Jefferson Chen, Jiajun Zhu, Trey Ideker, Zhiting Hu, Eric P. Xing</p>
            <p class="paper-venue">In submission to ISCB 2026</p>
            <p class="paper-description">A collaborative AI scientist agent for automated cell type annotation in single-cell transcriptomics analysis.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/nabla_2025.png" alt="Nabla Reasoner" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">Nabla Reasoner: LLM Reasoning via Test-Time Gradient Descent in Textual Space</a></h3>
            <p class="paper-authors">Peihao Wang, Ruisi Cai, <strong>Zhen Wang</strong>, Hongyuan Mei, Qiang Liu, Pan Li, Zhangyang Wang</p>
            <p class="paper-venue">In submission to ICLR 2026</p>
            <p class="paper-description">A novel approach to LLM reasoning through test-time gradient descent operations in textual space.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/modal_cot_2025.png" alt="Modal-mixed CoT" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">Learning Modal-mixed Chain-of-thought Reasoning with Latent Embedding</a></h3>
            <p class="paper-authors">Yifei Shao, Kun Zhou, Mohammad Atif Quamar, Ziming Xu, Shibo Hao, <strong>Zhen Wang</strong>, Zhiting Hu, Biwei Huang</p>
            <p class="paper-venue">In submission to ICLR 2026</p>
            <p class="paper-description">Learning chain-of-thought reasoning that seamlessly mixes different modalities through latent embeddings.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/fig_llm-reasoner_colm2024.png" alt="LLM Reasoners" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2404.05221">LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</a></h3>
            <p class="paper-authors">Shibo Hao*, Yi Gu*, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, <strong>Zhen Wang</strong>, Zhiting Hu</p>
            <p class="paper-venue">COLM 2024 <span class="highlight">‚≠ê 2.3k+ GitHub Stars</span></p>
            <p class="paper-description">A library enabling LLMs to conduct complex reasoning with advanced algorithms, approaching multi-step reasoning as planning with world models and rewards.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2404.05221">PDF</a>
              <a href="https://github.com/maitrix-org/llm-reasoners">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/promptagent_iclr_2024.png" alt="PromptAgent" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2310.16427">PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization</a></h3>
            <p class="paper-authors">Xinyuan Wang*, Chenxi Li*, <strong>Zhen Wang</strong>*‚Ä†, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, Zhiting Hu</p>
            <p class="paper-venue">ICLR 2024</p>
            <p class="paper-description">First principled framework to formalize API-based prompt optimization as planning with state, action, and reward; first to benchmark exploration efficiency and show transferability of optimized prompts.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2310.16427.pdf">PDF</a>
              <a href="https://github.com/XinyuanWangCS/PromptAgent">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/rap_emnlp_2023.png" alt="RAP" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2305.14992">Reasoning with Language Model is Planning with World Model</a></h3>
            <p class="paper-authors">Shibo Hao*, Yi Gu*, Haodi Ma, Joshua Jiahua Hong, <strong>Zhen Wang</strong>, Daisy Zhe Wang, Zhiting Hu</p>
            <p class="paper-venue">EMNLP 2023 (Oral, Main) <span class="highlight red">Featured in State of AI Report 2023</span></p>
            <p class="paper-description">RAP reformulates LLM reasoning as a planning problem, incorporating external world models and principled planning for the best balance of exploration vs exploitation.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2305.14992.pdf">PDF</a>
              <a href="https://github.com/Ber666/llm-reasoners">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/toolkengpt_neurips_2023.png" alt="ToolkenGPT" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2305.11554">ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings</a></h3>
            <p class="paper-authors">Shibo Hao, Tianyang Liu, <strong>Zhen Wang</strong>, Zhiting Hu</p>
            <p class="paper-venue">NeurIPS 2023 <span class="highlight red">Oral (Top 2%)</span> <span class="highlight">Best Paper @ SoCal NLP 2023</span></p>
            <p class="paper-description">Augments LLMs with massive tools/APIs by representing tools as tokens ("toolken"), enabling tool calls as naturally as generating words. Super efficient‚Äîplugging in new tools is as easy as learning embeddings.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2305.11554.pdf">PDF</a>
              <a href="https://github.com/Ber666/ToolkenGPT">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/thinksum_acl_2023.png" alt="ThinkSum" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2210.01293">ThinkSum: Probabilistic Reasoning Over Sets Using Large Language Models</a></h3>
            <p class="paper-authors">Batu Ozturkler, Nikolay Malkin, <strong>Zhen Wang</strong>, Nebojsa Jojic</p>
            <p class="paper-venue">ACL 2023 (Main)</p>
            <p class="paper-description">A two-stage probabilistic inference paradigm to improve LLMs' reasoning over multiple objects through Think (retrieval) and Sum (aggregation), beating chain-of-thought on hard BIG-bench tasks.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2210.01293.pdf">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/fig_gpr_turing_machine.png" alt="GPT Turing Machine" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2303.14310">GPT Is Becoming a Turing Machine: Here Are Some Ways to Program It</a></h3>
            <p class="paper-authors">Ana Jojic, <strong>Zhen Wang</strong>, Nebojsa Jojic</p>
            <p class="paper-venue">ICLR 2024 AGI Workshop</p>
            <p class="paper-description">Through appropriate prompting, GPT models can perform iterative behaviors to execute (not just write) programs with loops, including algorithms like logical deduction, bubble sort, and LCS.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2303.14310">PDF</a>
            </div>
          </div>
        </div>

      </section>

      <!-- Topic 4 -->
      <section class="section topic-section" id="topic-4">
        <h2 class="section-title">Human-Aligned Learning & Evaluation</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/deeppersona_2025.png" alt="DeepPersona" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>*, Yufan Zhou*, Zhongyan Luo, Lyumanshan Ye, Adam Wood, Man Yao, Saab Mansour, Luoshang Pan</p>
            <p class="paper-venue">Spotlight at NeurIPS 2025 LAW Workshop; In submission to ICLR 2026</p>
            <p class="paper-description">A generative engine for creating deep synthetic personas that enable realistic human simulation at scale.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/firebench_2025.png" alt="FIRE-Bench" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">FIRE-Bench: Evaluating Research Agents on the Rediscovery of Scientific Insights</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>*, Fan Bai*, Zhongyan Luo*, Jinyan Su, Kaiser Sun, Weiqi Liu, Albert Chen, Jieyuan Liu, Kun Zhou, Claire Cardie, Mark Dredze, Eric P. Xing, Zhiting Hu</p>
            <p class="paper-venue">In submission to ICLR 2026</p>
            <p class="paper-description">A benchmark for reliably evaluating research agents on their ability to rediscover scientific insights.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/ArXiv_logo_2022.png" alt="Decentralized Arena">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2505.12808">Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models</a></h3>
            <p class="paper-authors">Yanbin Yin*, Kun Zhou*, <strong>Zhen Wang</strong>*, Xiangdong Zhang, Yifei Shao, Shibo Hao, Yi Gu, Jieyuan Liu, Somanshu Singla, Tianyang Liu, Eric P. Xing, Zhengzhong Liu, Haojian Jin, Zhiting Hu</p>
            <p class="paper-venue">In submission to ACL 2026</p>
            <p class="paper-description">Democratic LLM benchmarking where models judge each other for scalable and fair evaluation.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2505.12808">PDF</a>
              <a href="https://de-arena.maitrix.org/">Demo</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/drpo_emnlp_2024.png" alt="DRPO" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2411.08733">Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models</a></h3>
            <p class="paper-authors">Somanshu Singla*, <strong>Zhen Wang</strong>*‚Ä†, Tianyang Liu, Abdullah Ashfaq, Zhiting Hu, Eric P. Xing</p>
            <p class="paper-venue">EMNLP 2024 (Main, Long)</p>
            <p class="paper-description">First tuning-free method for self-aligning LLMs with human preferences through dynamic rewarding and prompt optimization.</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2411.08733">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/alexa_prize_figure.jpeg" alt="TacoBot" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2207.05223">Bootstrapping a User-Centered Task-Oriented Dialogue System</a></h3>
            <p class="paper-authors">Shijie Chen, Ziru Chen, Xiang Deng, Ashley Lewis, Lingbo Mo, Samuel Stevens, <strong>Zhen Wang</strong>, Xiang Yue, Tianshu Zhang, Yu Su, Huan Sun</p>
            <p class="paper-venue">Alexa Prize TaskBot Challenge 2021 <span class="highlight red">üèÜ 3rd Place Winner</span></p>
            <p class="paper-description">TacoBot, a task-oriented dialogue system for cooking and home improvement tasks. Proposes data augmentation methods including GPT-3 simulation to bootstrap neural dialogue systems into new domains.</p>
            <div class="paper-links">
              <a href="https://assets.amazon.science/9a/30/5e4931ec41d78abad730707ce95a/bootstrapping-a-user-centered-task-oriented-dialogue-system.pdf">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/sigdial_2023.png" alt="SIGDIAL 2023" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://aclanthology.org/2023.sigdial-1.29/">Roll Up Your Sleeves: Working with a Collaborative and Engaging Task-Oriented Dialogue System</a></h3>
            <p class="paper-authors">Lingbo Mo, Shijie Chen, Ziru Chen, Xiang Deng, Ashley Lewis, Sunit Singh, Samuel Stevens, Chang-You Tai, <strong>Zhen Wang</strong>, Xiang Yue, Tianshu Zhang, Yu Su, Huan Sun</p>
            <p class="paper-venue">SIGDIAL 2023</p>
            <p class="paper-description">A collaborative and engaging task-oriented dialogue system for multi-step cooking and home improvement tasks.</p>
            <div class="paper-links">
              <a href="https://aclanthology.org/2023.sigdial-1.29.pdf">PDF</a>
            </div>
          </div>
        </div>

      </section>

      <!-- Topic 5 -->
      <section class="section topic-section" id="topic-5">
        <h2 class="section-title">Scientific Foundation Models & AI Scientists</h2>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/scpilot_neurips_2025.png" alt="scPilot" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery</a></h3>
            <p class="paper-authors">Yiming Gao*, <strong>Zhen Wang</strong>*‚Ä†, Jefferson Chen, Mark Antkowiak, Mengzhou Hu, JungHo Kong, Dexter Pratt, Jieyuan Liu, Enze Ma, Zhiting Hu, Eric P. Xing</p>
            <p class="paper-venue">NeurIPS 2025</p>
            <p class="paper-description">The first omics-native reasoning agent that grounds LLMs in raw single-cell data for automated analysis and biological discovery.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/cellmaster_2025.png" alt="CellMaster.AI" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://zhenwang9102.github.io/">CellMaster.AI: A Collaborative AI Scientist Agent for Cell Type Annotation in Single-Cell Transcriptomics Analysis</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Yiming Gao, Jieyuan Liu, Mark Antkowiak, Enze Ma, Ding Bai, JungHo Kong, Mengzhou Hu, Dexter Pratt, Jefferson Chen, Jiajun Zhu, Trey Ideker, Zhiting Hu, Eric P. Xing</p>
            <p class="paper-venue">In submission to ISCB 2026</p>
            <p class="paper-description">A collaborative AI scientist agent for automated cell type annotation in single-cell transcriptomics analysis.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/">PDF (Coming Soon)</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/nature_2025.png" alt="Nature 2025" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://www.biorxiv.org/content/10.1101/2023.01.03.522354">Atlas-Guided Discovery of Transcription Factors for T Cell Programming</a></h3>
            <p class="paper-authors">H. Kay Chung, Cong Liu, ... <strong>Zhen Wang</strong>, Jieyuan Liu, Yiming Gao, Zhiting Hu, ... Wei Wang</p>
            <p class="paper-venue">Nature 2025 <span class="highlight red">Accepted</span></p>
            <p class="paper-description">Contributed TaijiChat, a Paper Copilot for multi-omics discovery of transcription factors for T cell programming.</p>
            <div class="paper-links">
              <a href="https://www.biorxiv.org/content/10.1101/2023.01.03.522354v6.full.pdf">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/bioarchive-logo.png" alt="MutationProjector" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://www.biorxiv.org/content/10.1101/2025.09.08.674723">A Foundation Model of Cancer Genotype Enables Precise Predictions of Therapeutic Response</a></h3>
            <p class="paper-authors">JungHo Kong, Ingoo Lee, ... Hannah Carter, <strong>Zhen Wang</strong>‚Ä†, Trey Ideker‚Ä†</p>
            <p class="paper-venue">Under Revision at Cancer Discovery</p>
            <p class="paper-description">The first cancer genomics foundation model for tumor mutation profiles, enabling precise predictions of therapeutic response.</p>
            <div class="paper-links">
              <a href="https://www.biorxiv.org/content/10.1101/2025.09.08.674723v1.full.pdf">PDF</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/graph_bioinformatics.png" alt="BioNEV" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/1906.05017">Graph Embedding on Biomedical Networks: Methods, Applications, and Evaluations</a></h3>
            <p class="paper-authors">Xiang Yue, <strong>Zhen Wang</strong>, Jingong Huang, Srinivasan Parthasarathy, Soheil Moosavinasab, Yungui Huang, Simon Lin, Wen Zhang, Ping Zhang, Huan Sun</p>
            <p class="paper-venue">Bioinformatics, Volume 36, Issue 4, February 2020</p>
            <p class="paper-description">Benchmarks 11 representative graph embedding methods on five important biomedical tasks, verifying effectiveness and providing general guidelines for their usage.</p>
            <div class="paper-links">
              <a href="https://academic.oup.com/bioinformatics/article/36/4/1241/5581350">PDF</a>
              <a href="https://github.com/xiangyue9607/BioNEV">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/x-clinrela_acl__2020.png" alt="X-MedRELA" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/2005.00889">Rationalizing Medical Relation Prediction from Corpus-level Statistics</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Jennifer Lee, Simon Lin, Huan Sun</p>
            <p class="paper-venue">ACL 2020 (Main, Long)</p>
            <p class="paper-description">A self-interpretable framework to rationalize neural relation prediction based on corpus-level statistics, inspired by human cognitive theory about recall and recognition, providing structured knowledge triplets as rationales.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/ACL2020_ZW_X_MedRELA.pdf">PDF</a>
              <a href="https://github.com/zhenwang9102/X-MedRELA">Code</a>
            </div>
          </div>
        </div>

        <div class="paper-card">
          <div class="paper-image">
            <img src="/images/surfcon_kdd_2019.png" alt="SurfCon" onerror="this.onerror=null;this.src='/images/ArXiv_logo_2022.png';">
          </div>
          <div class="paper-content">
            <h3 class="paper-title"><a href="https://arxiv.org/abs/1906.09285">SurfCon: Synonym Discovery on Privacy-Aware Clinical Data</a></h3>
            <p class="paper-authors"><strong>Zhen Wang</strong>, Xiang Yue, Soheil Moosavinasab, Yungui Huang, Simon Lin, Huan Sun</p>
            <p class="paper-venue">KDD 2019 (Research Track, Long, Oral)</p>
            <p class="paper-description">Discovers structured knowledge‚Äîsynonyms‚Äîfrom privacy-aware clinical text corpus, leveraging both surface form and context information to discover out-of-distribution synonyms.</p>
            <div class="paper-links">
              <a href="https://zhenwang9102.github.io/files/KDD2019_ZW_SurfCon_paper.pdf">PDF</a>
              <a href="https://github.com/zhenwang9102/SurfCon">Code</a>
            </div>
          </div>
        </div>

      </section>

    </div>

  </main>

  <footer class="footer">
    <div class="container">
      <p class="footer-credit">
        Last updated: January 2026 &nbsp;|&nbsp; 
        <a href="https://scholar.google.com/citations?hl=en&user=asBaytUAAAAJ&view_op=list_works" target="_blank">Google Scholar</a>
      </p>
    </div>
  </footer>

  <script>
    function switchView(view) {
      document.querySelectorAll('.view-toggle button').forEach(btn => btn.classList.remove('active'));
      event.target.classList.add('active');
      document.querySelectorAll('.view-section').forEach(section => section.classList.remove('active'));
      if (view === 'years') {
        document.getElementById('viewYears').classList.add('active');
        document.getElementById('topicNav').classList.remove('visible');
      } else {
        document.getElementById('viewTopics').classList.add('active');
        document.getElementById('topicNav').classList.add('visible');
      }
    }
  </script>
  
</body>

</html>
